# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sraY6QrXf5fik1ZNJ18Dqrwjg3aaeW6F
"""


# Commented out IPython magic to ensure Python compatibility.
from __future__ import division, print_function



import pandas as pd
from string import*
from keras.callbacks import ModelCheckpoint
from keras.layers.recurrent import LSTM
from keras.models import Sequential
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Model
import gensim
from gensim.test.utils import common_texts, get_tmpfile
from gensim.models import Word2Vec
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import os
import collections
import string
import sklearn
os.chdir('..')
import seaborn as sns
from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding
data =  pd.read_csv('/home/honeycomb/Desktop/Minor2/Dataset/RestaurantDataset.csv',encoding='ISO-8859-1')
print(data)

data.shape
print(data.shape)
data.SentimentValue.value_counts()
print("SentimentValues: \n", data.SentimentValue.value_counts())


# %matplotlib inline
sns.set(style="darkgrid")
ax = sns.countplot(x='SentimentValue',  data=data)
plt.show()

import re
def remove_punct(text):
    text_nopunct = ''
    text_nopunct = re.sub('[,\.!?@#*/]', '', text)
    return text_nopunct
data['Text_Clean'] = data['Tweet'].apply(lambda x: remove_punct(x))
print("The actual data after removing the punctuations is \n",data)

from nltk import word_tokenize, WordNetLemmatizer
tokens = [word_tokenize(sen) for sen in data.Text_Clean]

def lower_token(tokens): 
    return [w.lower() for w in tokens]    
    
lower_tokens = [lower_token(token) for token in tokens]

from nltk.corpus import stopwords
stoplist = stopwords.words('english')
def removeStopWords(tokens): 
    return [word for word in tokens if word not in stoplist]
filtered_words = [removeStopWords(sen) for sen in lower_tokens]
data['Text_Final'] = [' '.join(sen) for sen in filtered_words]
data['tokens'] = filtered_words
print("tokenized Data is :\n",data['tokens'])

from wordcloud import WordCloud
long_string = ','.join(list(data['Text_Final'].values))
wordcloud = WordCloud(background_color="white", max_words=5000, contour_width=3, contour_color='steelblue')# Generate a word cloud
wordcloud.generate(long_string)# Visualize the word cloud
wordcloud.to_image()
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

def plot_10_most_common_words(count_data, count_vectorizer):
    import matplotlib.pyplot as plt
    words = count_vectorizer.get_feature_names()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]
    
    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words)) 
    
    plt.figure(2, figsize=(15, 15/1.6180))
    plt.subplot(title='10 most common words')
    sns.set_context("notebook", font_scale=1.25, rc={"lines.linewidth": 2.5})
    sns.barplot(x_pos, counts, palette='husl')
    plt.xticks(x_pos, words, rotation=90) 
    plt.xlabel('words')
    plt.ylabel('counts')
    plt.show()

count_vectorizer = CountVectorizer(stop_words='english')
count_data = count_vectorizer.fit_transform(data['Text_Final'])
plot_10_most_common_words(count_data, count_vectorizer)

data_train, data_test = train_test_split(data,test_size=0.10,random_state=42)

all_training_words = [word for tokens in data_train["tokens"] for word in tokens]
training_sentence_lengths = [len(tokens) for tokens in data_train["tokens"]]
TRAINING_VOCAB = sorted(list(set(all_training_words)))
print("%s words total, with a vocabulary size of %s\n" % (len(all_training_words), len(TRAINING_VOCAB)))
print("Max sentence length is %s\n" % max(training_sentence_lengths))
plt.xlabel(all_training_words)
plt.ylabel(TRAINING_VOCAB)

all_test_words = [word for tokens in data_test["tokens"] for word in tokens]
test_sentence_lengths = [len(tokens) for tokens in data_test["tokens"]]
TEST_VOCAB = sorted(list(set(all_test_words)))
print("%s words total, with a vocabulary size of %s\n" % (len(all_test_words), len(TEST_VOCAB)))
print("Max sentence length is %s\n" % max(test_sentence_lengths))
plt.xlabel(all_test_words)
plt.ylabel(TEST_VOCAB)

word2vec_path = '/home/honeycomb/Desktop/Minor2/GoogleNews-vectors-negative300.bin.gz'
word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)


def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):

    if len(tokens_list)<1:
        return np.zeros(k)
    if generate_missing:
        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]
    else:
        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]
    length = len(vectorized)
    summed = np.sum(vectorized, axis=0)
    averaged = np.divide(summed, length)
    return averaged

def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):
    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))
    return list(embeddings)

training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)
MAX_SEQUENCE_LENGTH = 50
EMBEDDING_DIM = 300

tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)
tokenizer.fit_on_texts(data_train['Text_Final'].tolist())
training_sequences = tokenizer.texts_to_sequences(data_train['Text_Final'].tolist())
train_word_index = tokenizer.word_index
print('Found %s unique tokens.\n' % len(train_word_index))
train_cnn_data = pad_sequences(training_sequences, 
                               maxlen=MAX_SEQUENCE_LENGTH)

train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)
train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))
for word,index in train_word_index.items():
    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)
print(train_embedding_weights.shape)
test_sequences = tokenizer.texts_to_sequences(data_test["Text_Final"].tolist())
test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)

label_names = ["SentimentValue"]
y_train = data_train[label_names].values
x_train = train_cnn_data
x_test = test_cnn_data
y_test = data_test[label_names].values



def recurrent_nn(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):
    
    embedding_layer = Embedding(num_words,
                            embedding_dim,
                            weights=[embeddings],
                            input_length=max_sequence_length,
                            trainable=False)
    
    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')
    embedded_sequences = embedding_layer(sequence_input)

    lstm = LSTM(256)(embedded_sequences)
    
    x = Dense(128, activation='relu')(lstm)
    x = Dropout(0.2)(x)
    preds = Dense(labels_index, activation='sigmoid')(x)

    model = Model(sequence_input, preds)
    model.compile(loss='mean_squared_error',
              optimizer='sgd',
              metrics=['mae'])
  
    
    model.summary()
    return model

model = recurrent_nn(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, 
                len(list(label_names)))

num_epochs = 10
hist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True)

predictions = model.predict(test_cnn_data, batch_size = 1024, verbose=1)
labels = [0, 2, 4]
prediction_labels=[]
for p in predictions:
    prediction_labels.append(labels[np.argmax(p)])
print(sum(data_test.SentimentValue==prediction_labels)/len(prediction_labels),'\n')
scores = model.evaluate(x_test, y_test, verbose=0)

print("Mean Absolute Error:", scores)






